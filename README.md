# Open Weather ETL Pipeline

Este projeto realiza a ingestÃ£o, transformaÃ§Ã£o e disponibilizaÃ§Ã£o de dados meteorolÃ³gicos obtidos por meio da API da [OpenWeather](https://openweathermap.org/api). O objetivo Ã© construir um pipeline de engenharia de dados completo, orquestrado com Apache Airflow, armazenando os dados em um banco NoSQL (MongoDB) e disponibilizando visualizaÃ§Ãµes interativas via Streamlit.

---

## ğŸ“Œ Objetivo

> Automatizar a coleta e transformaÃ§Ã£o de dados climÃ¡ticos por meio de uma pipeline em batch que executa ciclos de ETL a cada hora, garantindo dados atualizados para visualizaÃ§Ã£o e anÃ¡lise..

---

## ğŸ§± Arquitetura

![Arquitetura do Projeto](docs/arquitetura.png)

---

```lua
/openweather-pyspark-nosql
â”œâ”€â”€ data
â”œâ”€â”€ processed
â”‚   â””â”€â”€ ano=2025
â”‚       â””â”€â”€ mes=05
â”‚           â”œâ”€â”€ dia=24
â”‚           â”‚   â””â”€â”€ hora=15
â”‚           â””â”€â”€ dia=25
â”‚               â”œâ”€â”€ hora=22
â”‚               â””â”€â”€ hora=23
â””â”€â”€ raw
|   â””â”€â”€ ano=2025
|       â””â”€â”€ mes=05
|           â””â”€â”€ dia=25
|               â”œâ”€â”€ hora=22
|               â””â”€â”€ hora=23
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ airflow
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ notebook
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ streamlit
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ app_teste.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ arquitetura.png
â”‚   â”œâ”€â”€ extract_task.jpg
â”‚   â”œâ”€â”€ load_task.jpg
â”‚   â”œâ”€â”€ mongodb_clima.jpg
â”‚   â”œâ”€â”€ mongodb.jpg
â”‚   â”œâ”€â”€ streamlit_dashboard1.jpg
â”‚   â”œâ”€â”€ streamlit_dashboard2.jpg
â”‚   â”œâ”€â”€ streamlit_dashboard.jpg
â”‚   â””â”€â”€ transform_task.jpg
â”œâ”€â”€ info.txt
â”œâ”€â”€ makefile
â”œâ”€â”€ mnt
â”‚   â”œâ”€â”€ airflow
â”‚   â”‚   â””â”€â”€ dags
â”‚   â”œâ”€â”€ notebook
â”‚   â”‚   â”œâ”€â”€ notebook_teste.ipynb
â”‚   â”‚   â”œâ”€â”€ query_mongo_data.ipynb
â”‚   â”‚   â””â”€â”€ teste_pyspark.ipynb
â”‚   â”œâ”€â”€ python
â”‚   â”‚   â””â”€â”€ __pycache__
â”‚   â””â”€â”€ python_scripts
â”‚       â”œâ”€â”€ coletor_weather.py
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ load_weather_mongo.py
â”‚       â””â”€â”€ transformador_weather.py
â”œâ”€â”€ README.md
â”œâ”€â”€ services
â”‚   â”œâ”€â”€ applications.yml
â”‚   â”œâ”€â”€ orchestration.yml
â”‚   â””â”€â”€ visualization.yml
â””â”€â”€ tests
    â””â”€â”€ app.py
```
## ğŸ”„ Fluxo de dados

O pipeline segue a abordagem de **ETL (Extract, Transform, Load)**:

1. **ExtraÃ§Ã£o**:
   - Os dados climÃ¡ticos sÃ£o coletados da OpenWeather API utilizando scripts em Python.
   - Essa etapa Ã© acionada automaticamente pelo **Apache Airflow**, que agenda a execuÃ§Ã£o da DAG a cada `@hour`.

2. **TransformaÃ§Ã£o**:
   - Os dados brutos salvos na pasta `raw/` sÃ£o tratados, normalizados e enriquecidos com **PySpark**.
   - O resultado da transformaÃ§Ã£o Ã© salvo no diretÃ³rio `processed/`.

3. **Carga**:
   - Os dados processados sÃ£o inseridos no banco de dados **MongoDB**, organizados por cidade e data.

4. **VisualizaÃ§Ã£o**:
   - Um aplicativo desenvolvido com **Streamlit** consome os dados do MongoDB para gerar dashboards interativos e atualizados automaticamente com cada execuÃ§Ã£o da pipeline.
     
---

## ğŸ“… Agendamento e OrquestraÃ§Ã£o com Airflow

- A orquestraÃ§Ã£o da pipeline Ã© feita utilizando o **Apache Airflow**.
- A DAG principal Ã© executada **automaticamente a cada hora (`@hour`)**, garantindo que o MongoDB e o dashboard estejam sempre atualizados com os dados mais recentes da API.
- Cada execuÃ§Ã£o da DAG Ã© composta por trÃªs etapas sequenciais:
  1. `extract_weather_data` â€“ Extrai os dados da OpenWeather API e salva em `raw/`.
  2. `transform_with_pyspark` â€“ Processa os dados com PySpark e salva em `processed/`.
  3. `load_to_mongodb` â€“ Carrega os dados transformados no MongoDB.

---

## âš™ï¸ Tecnologias Utilizadas

| Tecnologia  | Papel no projeto                         |
|-------------|-------------------------------------------|
| Python      | Scripts de extraÃ§Ã£o e lÃ³gica do ETL       |
| Airflow     | OrquestraÃ§Ã£o e agendamento das tarefas    |
| Docker      | ContainerizaÃ§Ã£o de todos os serviÃ§os      |
| PySpark     | TransformaÃ§Ã£o dos dados                   |
| MongoDB     | Banco NoSQL para persistÃªncia dos dados   |
| Streamlit   | Interface web para visualizaÃ§Ã£o dos dados |
| Jupyter     | Testes e anÃ¡lise exploratÃ³ria             |

---

### VariÃ¡veis exigidas
Este projeto depende de algumas credenciais sensÃ­veis que devem ser configuradas por meio de variÃ¡veis de ambiente. Essas variÃ¡veis sÃ£o carregadas automaticamente a partir de um arquivo .env localizado no services/.env

| VariÃ¡vel             | DescriÃ§Ã£o                                                                 | Onde utilizar                    |
|----------------------|---------------------------------------------------------------------------|----------------------------------|
| `OPENWEATHER_API_KEY`| Chave da API da [OpenWeather](https://home.openweathermap.org/api_keys)  | Utilizada na etapa de extraÃ§Ã£o   |
| `MONGO_URI`          | URI de conexÃ£o com o MongoDB (ex: `mongodb://user:pass@host:port/db`)     | Utilizada na etapa de carga (`LoadMongo`) |

## ğŸ³ ExecuÃ§Ã£o com Makefile e Docker

Para iniciar todos os serviÃ§os com make:

```bash
up:
	docker compose -f services/orchestration.yml -f services/applications.yml -f services/visualization.yml up -d

down:
	docker compose -f services/orchestration.yml -f services/applications.yml -f services/visualization.yml down

build:
	docker compose -f services/orchestration.yml -f services/applications.yml up -f services/visualization.yml up -d --build

streamlit:
	docker compose -f services/visualization.yml up -d

make up
make down
make build
```

