# Open Weather ETL Pipeline

Este projeto realiza a ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados meteorol√≥gicos obtidos por meio da API da [OpenWeather](https://openweathermap.org/api). O objetivo √© construir um pipeline de engenharia de dados completo, orquestrado com Apache Airflow, armazenando os dados em um banco NoSQL (MongoDB) e disponibilizando visualiza√ß√µes interativas via Streamlit.

---

## üìå Objetivo

> Automatizar a coleta e transforma√ß√£o de dados clim√°ticos por meio de uma pipeline em batch que executa ciclos de ETL a cada hora, garantindo dados atualizados para visualiza√ß√£o e an√°lise..

---

## üß± Arquitetura

![Arquitetura do Projeto](docs/arquitetura.png)

---

## üîÑ Fluxo de dados

O pipeline segue a abordagem de **ETL (Extract, Transform, Load)**:

1. **Extra√ß√£o**:
   - Os dados clim√°ticos s√£o coletados da OpenWeather API utilizando scripts em Python.
   - Essa etapa √© acionada automaticamente pelo **Apache Airflow**, que agenda a execu√ß√£o da DAG a cada `@hour`.

2. **Transforma√ß√£o**:
   - Os dados brutos salvos na pasta `raw/` s√£o tratados, normalizados e enriquecidos com **PySpark**.
   - O resultado da transforma√ß√£o √© salvo no diret√≥rio `processed/`.

3. **Carga**:
   - Os dados processados s√£o inseridos no banco de dados **MongoDB**, organizados por cidade e data.

4. **Visualiza√ß√£o**:
   - Um aplicativo desenvolvido com **Streamlit** consome os dados do MongoDB para gerar dashboards interativos e atualizados automaticamente com cada execu√ß√£o da pipeline.
     
---

## üìÖ Agendamento e Orquestra√ß√£o com Airflow

- A orquestra√ß√£o da pipeline √© feita utilizando o **Apache Airflow**.
- A DAG principal √© executada **automaticamente a cada hora (`@hour`)**, garantindo que o MongoDB e o dashboard estejam sempre atualizados com os dados mais recentes da API.
- Cada execu√ß√£o da DAG √© composta por tr√™s etapas sequenciais:
  1. `extract_weather_data` ‚Äì Extrai os dados da OpenWeather API e salva em `raw/`.
  2. `transform_with_pyspark` ‚Äì Processa os dados com PySpark e salva em `processed/`.
  3. `load_to_mongodb` ‚Äì Carrega os dados transformados no MongoDB.

---

## ‚öôÔ∏è Tecnologias Utilizadas

| Tecnologia  | Papel no projeto                         |
|-------------|-------------------------------------------|
| Python      | Scripts de extra√ß√£o e l√≥gica do ETL       |
| Airflow     | Orquestra√ß√£o e agendamento das tarefas    |
| Docker      | Containeriza√ß√£o de todos os servi√ßos      |
| PySpark     | Transforma√ß√£o dos dados                   |
| MongoDB     | Banco NoSQL para persist√™ncia dos dados   |
| Streamlit   | Interface web para visualiza√ß√£o dos dados |
| Jupyter     | Testes e an√°lise explorat√≥ria             |

---

## üê≥ Execu√ß√£o com Makefile e Docker

Para iniciar todos os servi√ßos com make:

```bash
up:
	docker compose -f services/orchestration.yml -f services/applications.yml -f services/visualization.yml up -d

down:
	docker compose -f services/orchestration.yml -f services/applications.yml -f services/visualization.yml down

build:
	docker compose -f services/orchestration.yml -f services/applications.yml up -f services/visualization.yml up -d --build

streamlit:
	docker compose -f services/visualization.yml up -d

make up
make down
make build
```

